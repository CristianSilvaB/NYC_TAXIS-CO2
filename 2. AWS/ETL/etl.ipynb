{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***ETL automatizado en AWS***\n",
    "\n",
    "Los datasets cargados en un bucket de **S3** fueron sometidos a un proceso de *extracción, transformación y carga* (`ETL` por sus siglas en inglés) usando `jobs` de **AWS Glue** escritos en `PySpark`. El proceso fue programado para realizarse el dia 15 de cada mes a las 00:00hs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Script de `sound_quality`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Obtener argumentos del trabajo de Glue\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'BUCKET_NAME', 'INPUT_FILE_KEY', 'OUTPUT_FILE_KEY'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "BUCKET_NAME = args['BUCKET_NAME']\n",
    "INPUT_FILE_KEY = args['INPUT_FILE_KEY']\n",
    "OUTPUT_FILE_KEY = args['OUTPUT_FILE_KEY']\n",
    "\n",
    "def main():\n",
    "    # Leer el archivo CSV directamente con Spark\n",
    "    input_path = f's3://{BUCKET_NAME}/{INPUT_FILE_KEY}'\n",
    "    annotations = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Seleccionar las columnas necesarias\n",
    "    df_sound = annotations.select('borough', 'latitude', 'longitude', 'year', 'day', 'hour', '1_engine_presence')\n",
    "\n",
    "    # Renombrar la columna '1_engine_presence'\n",
    "    df_sound = df_sound.withColumnRenamed('1_engine_presence', 'Engine Sound')\n",
    "    \n",
    "    # Mapeo de valores\n",
    "    df_sound = df_sound.withColumn(\n",
    "        'Engine Sound', \n",
    "        when(col('Engine Sound') == -1, 'Low')\n",
    "        .when(col('Engine Sound') == 0, 'Medium')\n",
    "        .when(col('Engine Sound') == 1, 'High')\n",
    "    )\n",
    "\n",
    "    # Renombrar otras columnas\n",
    "    df_sound = df_sound.withColumnRenamed('borough', 'Borough') \\\n",
    "                       .withColumnRenamed('latitude', 'Latitude') \\\n",
    "                       .withColumnRenamed('longitude', 'Longitude') \\\n",
    "                       .withColumnRenamed('year', 'Year') \\\n",
    "                       .withColumnRenamed('day', 'Day') \\\n",
    "                       .withColumnRenamed('hour', 'Hour')\n",
    "\n",
    "    # Escribir el DataFrame en formato Parquet a S3\n",
    "    output_path = f's3://{BUCKET_NAME}/{OUTPUT_FILE_KEY}'\n",
    "    df_sound.write.mode('overwrite').parquet(output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Script de `electric_alternative_fuel_stations`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "# Obtener argumentos del trabajo de Glue\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'BUCKET_NAME', 'INPUT_FILE_KEY', 'OUTPUT_FILE_KEY'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "BUCKET_NAME = args['BUCKET_NAME']\n",
    "INPUT_FILE_KEY = args['INPUT_FILE_KEY']\n",
    "OUTPUT_FILE_KEY = args['OUTPUT_FILE_KEY']\n",
    "\n",
    "def main():\n",
    "    # Leer el archivo CSV directamente con Spark\n",
    "    input_path = f's3://{BUCKET_NAME}/{INPUT_FILE_KEY}'\n",
    "    df_stations = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Seleccionar las columnas necesarias\n",
    "    columnas = ['Fuel Type Code', 'Station Name', 'Street Address', 'City', 'State', 'Latitude', 'Longitude']\n",
    "    fuel_stations = df_stations.select(*columnas)\n",
    "\n",
    "    # Filtrar por el estado de Nueva York\n",
    "    fuel_stations = fuel_stations.filter(fuel_stations['State'] == 'NY')\n",
    "\n",
    "    # Filtrar por los 5 distritos principales\n",
    "    boroughs = ['Brooklyn', 'Manhattan', 'Queens', 'Bronx', 'Staten Island']\n",
    "    fuel_stations = fuel_stations.filter(fuel_stations['City'].isin(boroughs))\n",
    "\n",
    "    # Renombrar columna 'City' a 'Borough'\n",
    "    fuel_stations = fuel_stations.withColumnRenamed('City', 'Borough')\n",
    "\n",
    "    # Eliminar la columna 'State'\n",
    "    fuel_stations = fuel_stations.drop('State')\n",
    "\n",
    "    # Escribir el DataFrame en formato Parquet a S3\n",
    "    output_path = f's3://{BUCKET_NAME}/{OUTPUT_FILE_KEY}'\n",
    "    fuel_stations.write.mode('overwrite').parquet(output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Script de `taxi_zones`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "BUCKET_NAME = args['BUCKET_NAME']\n",
    "INPUT_FILE_KEY = args['INPUT_FILE_KEY']\n",
    "OUTPUT_FILE_KEY = args['OUTPUT_FILE_KEY']\n",
    "\n",
    "def main():\n",
    "    # Descargar el archivo DBF desde S3\n",
    "    obj = s3_client.get_object(Bucket=BUCKET_NAME, Key=INPUT_FILE_KEY)\n",
    "    dbf_content = obj['Body'].read()\n",
    "\n",
    "    # Guardar el contenido del archivo DBF temporalmente\n",
    "    temp_dbf_path = \"/tmp/taxi_zones.dbf\"\n",
    "    with open(temp_dbf_path, \"wb\") as temp_dbf:\n",
    "        temp_dbf.write(dbf_content)\n",
    "\n",
    "    # Leer el archivo DBF con pandas\n",
    "    from dbfread import DBF\n",
    "    taxi_zones = DBF(temp_dbf_path)\n",
    "    taxi_zones_df = pd.DataFrame(taxi_zones)\n",
    "\n",
    "    # Convertir el DataFrame de pandas a Spark DataFrame\n",
    "    taxi_zones_spark_df = spark.createDataFrame(taxi_zones_df)\n",
    "\n",
    "    # Eliminar columnas innecesarias\n",
    "    innecesarias = ['Shape_Area', 'Shape_Leng']\n",
    "    taxi_zones_spark_df = taxi_zones_spark_df.drop(*innecesarias)\n",
    "\n",
    "    # Renombrar columnas\n",
    "    taxi_zones_spark_df = taxi_zones_spark_df.withColumnRenamed('zone', 'Zone') \\\n",
    "                                             .withColumnRenamed('OBJECTID', 'ObjectID') \\\n",
    "                                             .withColumnRenamed('borough', 'Borough')\n",
    "\n",
    "    # Eliminar la columna 'ObjectID'\n",
    "    taxi_zones_spark_df = taxi_zones_spark_df.drop('ObjectID')\n",
    "\n",
    "    # Eliminar duplicados\n",
    "    taxi_zones_spark_df = taxi_zones_spark_df.dropDuplicates()\n",
    "\n",
    "    # Escribir el DataFrame en formato Parquet a S3\n",
    "    output_path = f's3://{BUCKET_NAME}/{OUTPUT_FILE_KEY}'\n",
    "    taxi_zones_spark_df.write.mode('overwrite').parquet(output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Script de `vehicle_fuel_economy_data`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Obtener argumentos del trabajo de Glue\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'BUCKET_NAME', 'INPUT_FILE_KEY', 'OUTPUT_FILE_KEY'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "BUCKET_NAME = args['BUCKET_NAME']\n",
    "INPUT_FILE_KEY = args['INPUT_FILE_KEY']\n",
    "OUTPUT_FILE_KEY = args['OUTPUT_FILE_KEY']\n",
    "\n",
    "def main():\n",
    "    # Leer el archivo CSV directamente con Spark\n",
    "    input_path = f's3://{BUCKET_NAME}/{INPUT_FILE_KEY}'\n",
    "    vfed = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Seleccionar las columnas necesarias\n",
    "    vfed = vfed.select('Model', 'Year', 'Manufacturer', 'VClass', 'fuelType1', 'fuelType2', 'city08', 'co2')\n",
    "\n",
    "    # Renombrar columnas\n",
    "    vfed = vfed.withColumnRenamed('VClass', 'Category') \\\n",
    "               .withColumnRenamed('fuelType1', 'Fuel') \\\n",
    "               .withColumnRenamed('fuelType2', 'Alternative Fuel') \\\n",
    "               .withColumnRenamed('city08', 'Consumption (mpg)') \\\n",
    "               .withColumnRenamed('co2', 'CO2 (g/mile)')\n",
    "\n",
    "    # Reemplazar valores nulos en la columna 'Fuel' con 'No'\n",
    "    vfed = vfed.fillna({'Fuel': 'No'})\n",
    "\n",
    "    # Reemplazar valores en la columna 'Fuel'\n",
    "    vfed = vfed.replace(['Gasoline', 'Electricity'], ['Gasoline', 'Electric'], subset='Fuel')\n",
    "\n",
    "    # Eliminar filas duplicadas basadas en todas las columnas\n",
    "    vfed = vfed.dropDuplicates()\n",
    "\n",
    "    # Filtrar las categorías necesarias\n",
    "    categories = ['Small Sport Utility Vehicle 4WD', 'Small Sport Utility Vehicle 2WD', \n",
    "                  'Compact Cars', 'Midsize Cars', 'Large Cars', 'Standard Sport Utility Vehicle 4WD',\n",
    "                  'Standard Sport Utility Vehicle 2WD', 'Minivan - 2WD', 'Vans', 'Minivan - 4WD']\n",
    "    vfed = vfed.filter(vfed['Category'].isin(categories))\n",
    "\n",
    "    # Escribir el DataFrame en formato Parquet a S3\n",
    "    output_path = f's3://{BUCKET_NAME}/{OUTPUT_FILE_KEY}'\n",
    "    vfed.write.mode('overwrite').parquet(output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
