{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Web Scrapping en AWS***\n",
    "\n",
    "Decidimos hacer web scrapping con AWS Lambda a la página \"NYC Taxi & Limousine Commision\" para extraer los datasets correspondientes a la información sobre los viajes de los taxis verdes y amarillos realizados durante el año 2023 para luego ingestar estos datos en un bucket de S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import boto3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import chromedriver_binary\n",
    "\n",
    "# Configurar S3\n",
    "s3_client = boto3.client('s3')\n",
    "BUCKET_NAME = 'tu-bucket-de-s3'\n",
    "FOLDER_NAME = 'carpeta-en-s3'\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        # Configurar el navegador\n",
    "        opts = Options()\n",
    "        opts.add_argument(\"--headless\")\n",
    "        opts.add_argument(\"--no-sandbox\")\n",
    "        opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "        opts.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install(), options=opts)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "        driver.get(url)\n",
    "\n",
    "        tabla_celdas = driver.find_elements(By.XPATH, \"//div[@id='faq2023']//table//tr//td\")\n",
    "\n",
    "        def obtenerLinks(celdas):\n",
    "            for td in celdas:\n",
    "                listas = td.find_elements(By.TAG_NAME, \"ul\")\n",
    "                for elemento_lista in listas:\n",
    "                    links_columns = elemento_lista.find_elements(By.TAG_NAME, \"a\")\n",
    "                    for link in links_columns:\n",
    "                        titulo = link.get_attribute(\"title\")\n",
    "                        if titulo == \"Yellow Taxi Trip Records\" or titulo == \"Green Taxi Trip Records\":\n",
    "                            enlace = link.get_attribute(\"href\")\n",
    "                            nombre_archivo = enlace.split(\"/\")[-1]\n",
    "                            response = requests.get(enlace, stream=True)\n",
    "                            if response.status_code == 200:\n",
    "                                s3_key = f\"{FOLDER_NAME}/{nombre_archivo}\"\n",
    "                                s3_client.upload_fileobj(response.raw, BUCKET_NAME, s3_key)\n",
    "                                print(f\"Archivo '{nombre_archivo}' guardado en S3 en '{s3_key}'\")\n",
    "                            else:\n",
    "                                print(f\"Error al descargar el archivo: {enlace}\")\n",
    "\n",
    "        obtenerLinks(tabla_celdas)\n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            'statusCode': 200, # Indica éxito\n",
    "            'body': json.dumps('Scraping y almacenamiento en S3 completado')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'statusCode': 500,  # Indica error del servidor\n",
    "            'body': json.dumps(f'Error: {str(e)}')\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
